<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Infomall.org â€“ Projects</title>
    <link>https://infomall.org/uva/docs/projects/</link>
    <description>Recent content in Projects on Infomall.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 20 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="https://infomall.org/uva/docs/projects/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Cylon</title>
      <link>https://infomall.org/uva/docs/projects/cylon/</link>
      <pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/cylon/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;One of the most exciting aspects of the Big Data era for both the industry and research communities is the incredible progress being made in the domains of the machine and deep learning. Modern applications demand resources that are more than a single node can supply.
The difficulties that the total data processing environment must address include a variety of data engineering for pre- and post-data processing, communication, and system integration. The ability of data analytics tools to quickly interface with existing frameworks in a variety of languages is a crucial requirement as it increases user productivity and efficiency.All of this calls for an effective and widely dispersed integrated approach to data processing, yet many of today&amp;rsquo;s well-liked data analytics solutions are unable to simultaneously meet all of these criteria.&lt;/p&gt;
&lt;p&gt;In this project, we introduce Cylon, a high-performance distributed data processing toolkit that is open-source and easily integrated with current Big Data and AI/ML frameworks. It has a compact data structure as the foundation, a versatile C++ core, and language bindings for Python, Java, and C++ on top of it.&lt;/p&gt;
&lt;p&gt;We develop Cylon&amp;rsquo;s design and demonstrate how it can be used as a standalone framework or imported as a library into already-existing applications. Early tests reveal that Cylon boosts well-known technologies like Apache Spark and Dask with significant performance gains for crucial operations and improved component linkages. The ultimate goal is to demonstrate how Cylon&amp;rsquo;s design supports cross-platform usage with the least amount of overhead, which includes well-known AI tools like PyTorch, Tensorflow, and Jupyter notebooks.&lt;/p&gt;

&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Cosmoflow</title>
      <link>https://infomall.org/uva/docs/projects/cosmoflow/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/cosmoflow/</guid>
      <description>
        
        
        &lt;h2 id=&#34;high-performance-computing-with-the-cosmoflow-benchmark&#34;&gt;High Performance Computing with the Cosmoflow Benchmark&lt;/h2&gt;
&lt;p&gt;CosmoFlow is a deep learning application that uses a convolutional neural network to predict the large-scale structure of the universe from cosmological simulations. Developed by researchers from Oak Ridge National Laboratory and NVIDIA, the project achieved a 4x speedup compared to traditional methods by leveraging GPUs and high-performance computing. CosmoFlow generates large datasets that researchers can analyze to gain insights into the evolution of the cosmos. The neural network is trained on 3D images generated by cosmological simulations, and the resulting datasets provide valuable information about the formation of galaxies, distribution of dark matter, and overall evolution of the universe. CosmoFlow enables faster and more efficient simulations, opening up new avenues of research in this area. The project has the potential to revolutionize cosmology research by enabling faster and more accurate simulations of the universe.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/DSC-SPIDAL/mlcommons-cosmoflow&#34;&gt;https://github.com/DSC-SPIDAL/mlcommons-cosmoflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mlcommons/hpc&#34;&gt;https://github.com/mlcommons/hpc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;report (TBD)&lt;/li&gt;
&lt;li&gt;paper (TBD)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MLcommons repository of cosmoflow, &lt;a href=&#34;https://github.com/mlcommons/hpc/tree/main/cosmoflow&#34;&gt;https://github.com/mlcommons/hpc/tree/main/cosmoflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DSC respository of cosmoflow, &lt;a href=&#34;https://github.com/DSC-SPIDAL/mlcommons-cosmoflow&#34;&gt;https://github.com/DSC-SPIDAL/mlcommons-cosmoflow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Cylon on AWS</title>
      <link>https://infomall.org/uva/docs/projects/cyloncloud/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/cyloncloud/</guid>
      <description>
        
        
        &lt;h2 id=&#34;high-performance-data-engineering-with-cylon-on-amazon-web-services&#34;&gt;High Performance Data Engineering with Cylon on Amazon Web Services&lt;/h2&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;In recent years the data engineering discipline has been greatly impacted by Artificial Intelligence (AI) and Machine Learning (ML). The effect has ushered in research related to the speed, performance, and optimization of such processes [2]. To meet these ends, many frameworks have been proposed.  One such framework is CylonData [1].  CylonData represents an architecture where performance critical operations are moved to a highly optimized library. Moreover, the architecture provides the capability to leverage the performance associated with in-memory data and distributed operations and data across processes, a key requirement related to processing large data engineering workloads at scale. Such benefits are realized, for example, in the conversion from tabular or table format to tensor format required for ML/DL or via the use of relational algebraic expressions such as joins, select, project, etc. More specifically Cylon is described as &amp;ldquo;a fast, scalable distributed memory data parallel library for processing structured data&amp;rdquo; [1].  While CylonData has focused on a MPI implementation using HPC ML resources, the research work here is to port this to a serverless compute infrastructure within AWS services such as AWS Lambda, ECS, EC2, Route 53, ALB, and EFS.  Once this is completed we will be achieving two things. First, we will be showcasing this work will be available not only on HPC but also on AWS serverless and serverful compute resources.  Second, we will be able to provide an extensive benchmark comparison between HPC and Serverless/Serverful Computing to showcase the strengths and weaknesses of both approaches.&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cylondata/cylon&#34;&gt;https://github.com/cylondata/cylon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;report (TBD)&lt;/li&gt;
&lt;li&gt;paper (TBD)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1, April 2023: Merge of UCC/UCX Bootstrapping to cylondata/man&lt;/li&gt;
&lt;li&gt;25, April 2023: Execution of Serverful Cylon Delivered via ECS Infrastructure using OpenMPI&lt;/li&gt;
&lt;li&gt;10, May 2023: Prototype of Servlerless delivery of cylon library via AWS Lambda Layers&lt;/li&gt;
&lt;li&gt;30, May 2023: Serverless Cache infrastructure fascade prototype and support via abstraction in the cylon source&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Cylon.&amp;rdquo; cylondata/cylon, &lt;a href=&#34;https://github.com/cylondata/cylon/&#34;&gt;https://github.com/cylondata/cylon/&lt;/a&gt;.  Accessed 9 September 2022.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Cylon Library for Fast &amp;amp; Scalable Data Engineering.&amp;rdquo; Cylon Blog, &lt;a href=&#34;https://supun-kamburugamuve.medium.com/cylon-library-for-fast-scalable-data-engineering-bf74742fe5d1&#34;&gt;https://supun-kamburugamuve.medium.com/cylon-library-for-fast-scalable-data-engineering-bf74742fe5d1&lt;/a&gt;.  Accessed 9 September 2022.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Data Compiler</title>
      <link>https://infomall.org/uva/docs/projects/data-compiler/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/data-compiler/</guid>
      <description>
        
        
        &lt;h2 id=&#34;optimizing-large-scale-deep-learning-by-data-movement-aware-compiler&#34;&gt;Optimizing Large-Scale Deep Learning by Data Movement-Aware Compiler&lt;/h2&gt;
&lt;p&gt;Projet Members&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://luosuu.github.io/&#34;&gt;Tianle Zhong&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-summary&#34;&gt;Project Summary&lt;/h2&gt;
&lt;p&gt;This project aims to address the data movement, as a known major efficiency bottleneck of distributed training[1], by designing a tensor compiler[2] which can acquire and optimize the data movement graph and scheduling at the compilation time so that the execution becomes fully static for higher performance. Such Ahead-of-Time(AOT) optimization also enables opportunities for auto-parallelism and pipelining like [3, 5]. The current exploration is about leveraging Multi-Level Intermediate Representation (MLIR)[4] to include data movement information into the compilation passes.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Proposal: WIP&lt;/li&gt;
&lt;li&gt;GitHub repo: not ready for open access&lt;/li&gt;
&lt;li&gt;Report: TBD&lt;/li&gt;
&lt;li&gt;Paper: TBD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;15 April 2023: Define a MLIR dialect which can describe data movement of deep learning&lt;/li&gt;
&lt;li&gt;1 June 2023: Figure out the optimization over the dialect converting and lowering passes&lt;/li&gt;
&lt;li&gt;15 July 2023: Prototype the backend code generation for real-world tests&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Ivanov, Andrei, et al. &amp;ldquo;Data movement is all you need: A case study on optimizing transformers.&amp;rdquo; &lt;em&gt;Proceedings of Machine Learning and Systems&lt;/em&gt; 3 (2021): 711-732. available at &lt;a href=&#34;https://arxiv.org/abs/2007.00072&#34;&gt;https://arxiv.org/abs/2007.00072&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kjolstad, Fredrik, et al. &amp;ldquo;The tensor algebra compiler.&amp;rdquo; &lt;em&gt;Proceedings of the ACM on Programming Languages&lt;/em&gt; 1.OOPSLA (2017): 1-29. available at &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3133901&#34;&gt;https://dl.acm.org/doi/10.1145/3133901&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Yuan, Jinhui, et al. &amp;ldquo;Oneflow: Redesign the distributed deep learning framework from scratch.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2110.15032&#34;&gt;https://arxiv.org/abs/2110.15032&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vasilache, Nicolas, et al. &amp;ldquo;Composable and modular code generation in MLIR: A structured and retargetable approach to tensor compiler construction.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2202.03293&#34;&gt;https://arxiv.org/abs/2202.03293&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zheng, Lianmin, et al. &amp;ldquo;Alpa: Automating Inter-and {Intra-Operator} Parallelism for Distributed Deep Learning.&amp;rdquo; &lt;em&gt;16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)&lt;/em&gt;. available at &lt;a href=&#34;https://arxiv.org/abs/2201.12023&#34;&gt;https://arxiv.org/abs/2201.12023&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: NIST</title>
      <link>https://infomall.org/uva/docs/projects/nist/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/nist/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Over the last several years, the computation landscape for conducting
data an- alytics has completely changed. While in the past a lot of
the activities have been undertaken in isolation by companies and
research institutions, todayâ€™s in- frastructure constitutes a wealth
of services offered by a variety of providers that offer opportunities
for reuse and interactions.&lt;/p&gt;
&lt;p&gt;We will expand analytics services to focus on developing a frame- work
for reusable hybrid multi-service data analytics. It includes (a) a
technology review that explicitly targets the intersection of hybrid
multi-provider analytics services (b) enhancing the concepts of
services to showcase how hybrid, as well as multi-provider services,
can be integrated and reused via the proposed framework, (d) address
analytics service composition, and (c) integrate container
technologies to achieve state-of-the-art analytics service deployment
capabilities.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;PI: Gregor von Laszewski, &lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: OSMI</title>
      <link>https://infomall.org/uva/docs/projects/osmi/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/osmi/</guid>
      <description>
        
        
        &lt;h2 id=&#34;mlcommons-osmi-benchmark&#34;&gt;MLCommons OSMI Benchmark&lt;/h2&gt;
&lt;p&gt;OSMI-Bench explores the optimal deployment of machine-learned surrogate (MLS) models in rotorcraft aerodynamics on high-performance computers (HPC). In this benchmark, we test three rotorcraft models for optimal deployment configurations, including, Long Short Term Memory (LSTM), Convolutional Neural Network (CNN), and Temporal Convolutional Neural Network (TCNN) models with 2M, 44M, and 212M trainable parameters respectively [1]. Surrogate models trained on synthetic data were selected because we are solely focused on inference efficiency not model accuracy. We are now running the benchmark on the Rivanna HPC at the University of Virginia to find the optimal deployment scenario for each model, and we plan to develop more models to benchmark, such as a transformer-encoder natural language model. We are also investigating the relationship between batchsize, GPU, and concurrency and inference throughput/time. We will soon explore running the load balancers used in the OSMI-Bench framework, such as Python concurrent.futures threadpool, HAProxy and mpi4py, on Rivanna.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;(gitrepo)[https://github.com/laszewsk/osmi]&lt;/li&gt;
&lt;li&gt;report (TBD)&lt;/li&gt;
&lt;li&gt;paper (TBD)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;31 March 2023: Graph the relationship between configuration and inference performance for each model, complete OSMI-Bench documentation for Rivanna&lt;/li&gt;
&lt;li&gt;15 April: Run load balancer on Rivanna&lt;/li&gt;
&lt;li&gt;1 May 2023: Develop and benchmark new models&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Wesley Brewer et al. &amp;ldquo;Production Deployment of Machine-Learned Rotorcraft Surrogate Models on HPC&amp;rdquo;, 2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC), 15 November 2021, 10.1109/MLHPC54614.2021.00008, [https://ieeexplore.ieee.org/document/9652868]&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;participants&#34;&gt;Participants&lt;/h2&gt;
&lt;p&gt;Nate Kimball, Wes Brewer, Gregor von Laszewski (&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;), Geoffrey C. Fox&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
